{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Performance by Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we investigate the performance of a basic autoencoder model iwhen neighbors become random in the high-dimensional space according to the data size. We start by loading the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling arrays\n",
    "import numpy as np\n",
    "\n",
    "# Handling data frames\n",
    "import pandas as pd\n",
    "\n",
    "# neural networks in Python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntimes = 100 # number of noise replicates to investigate dimred performance\n",
    "npoints = [25, 50, 100] # number of points in our ground truth data set to be investigated\n",
    "maxdim = 10000 # maximal dimension of the data set to be investigated\n",
    "dims = np.round(np.exp(np.linspace(np.log(2), np.log(maxdim), num=10))).astype(\"int\") # dimensions to study\n",
    "a = 1.25 # magnitude of noise: per dimension we sample noise uniformly from [-a, a]\n",
    "alpha = 3 # factor controling the growth rate of the ground truth diamete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We construct the ground truth data sets according to the various growth rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for idx, points in enumerate(npoints):\n",
    "    t = np.linspace(0, 1, num=points)\n",
    "    factor = np.ones(maxdim) if alpha == np.inf else (np.arange(maxdim) + 1)**(-1 / alpha)\n",
    "    datasets.append(np.transpose(np.tile(t, (maxdim, 1))) * np.tile(factor, (points, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, gain=1.0)        \n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.encoding_dim = encoding_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 24),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(24, 6),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, self.encoding_dim),\n",
    "            nn.Tanh())\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.encoding_dim, 6),\n",
    "            nn.Tanh(), \n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6, 24),\n",
    "            nn.Linear(24, self.input_dim))\n",
    "        \n",
    "        self.encoder.apply(self.init_weights)\n",
    "        self.decoder.apply(self.init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def autoencode(X, encoding_dim, num_epochs=2000, learning_rate=1e-3, eps=1e-07):\n",
    "    X = torch.tensor(X).type(torch.float)\n",
    "    model = autoencoder(input_dim=X.shape[1], encoding_dim=encoding_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, eps=eps)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    Y = model.encoder(X).detach().numpy()\n",
    "    \n",
    "    return(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the autoencoder performance by dimensionality and data size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 98.00%\r"
     ]
    }
   ],
   "source": [
    "cor_auto = np.zeros([len(dims), len(npoints)])\n",
    "\n",
    "for idx in range(ntimes):\n",
    "    \n",
    "    print(\"progress: \" + str(round(100 * idx / ntimes, 2)).ljust(5, \"0\") + \"%\", end=\"\\r\")\n",
    "    \n",
    "    N = a * (2 * np.random.rand(max(npoints), maxdim) - 1)\n",
    "    \n",
    "    for np_idx, points in enumerate(npoints):\n",
    "        \n",
    "            XN = datasets[np_idx] + a * N[:datasets[np_idx].shape[0],]\n",
    "            \n",
    "            for dim_idx, dim in enumerate(dims):\n",
    "                \n",
    "                Y = autoencode(XN[:,:dim], 1)\n",
    "                cor = np.max([np.corrcoef(Y[:,0], datasets[np_idx][:,0])[0, 1], \n",
    "                              np.corrcoef(np.flip(Y[:,0], axis=0), datasets[np_idx][:,0])[0, 1]])\n",
    "                cor_auto[dim_idx, np_idx] += cor\n",
    "                \n",
    "cor_auto /= ntimes\n",
    "\n",
    "print(\"progress: 100.0%\", end=\"\\r\")\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.set_xlabel(\"dim\")\n",
    "ax.set_ylabel(\"correlation\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "for idx, points in enumerate(npoints):\n",
    "    \n",
    "    ax.plot(dims, cor_auto[:,idx], label=points)\n",
    "    ax.scatter(dims, cor_auto[:,idx])\n",
    "    \n",
    "ax.legend(title=\"data size\", loc=\"upper center\", ncol=3, bbox_to_anchor=(0.5, 1.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the style of the results and save as csv for plotting in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_auto_df = np.zeros([cor_auto.shape[0] * cor_auto.shape[1], 3])\n",
    "idx = 0\n",
    "for idx1, points in enumerate(npoints):\n",
    "    for idx2, dim in enumerate(dims):\n",
    "        cor_auto_df[idx,:] = [points, dim, cor_auto[idx2, idx1]]\n",
    "        idx += 1 \n",
    "        \n",
    "cor_auto_df = pd.DataFrame(cor_auto_df)\n",
    "cor_auto_df.columns = [\"size\", \"dim\", \"cor\"]\n",
    "cor_auto_df.to_csv(\"../Results/Size/AUTO_alpha3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
